#jinja2: trim_blocks:True, lstrip_blocks:True
# NHC Configuration File
#
# Lines are in the form "<hostmask>||<check>"
# Hostmask is a glob, /regexp/, or {noderange}
# Comments begin with '#'
#


#######################################################################
###
### NHC Configuration Variables
###
# Explicitly instruct NHC to assume PBS (TORQUE, PBSPro) is the Resource Manager
   * || export NHC_RM=slurm

# Do not mark nodes offline
#   * || export MARK_OFFLINE=0

# Activate debugging mode
#   * || export DEBUG=1

# Set watchdog timer to 15 seconds
#   * || export TIMEOUT=15

# In out-of-band contexts, enable all checks
#   * || export NHC_CHECK_ALL=1

# Run df only for local file systems excluding various "special" file system types.
# This prevents running df for large shared file systems resulting in
# "NHC: Watchdog timer unable to terminate hung NHC process" errors.
   * || export DFI_FLAGS='-T -i -l -x tmpfs -x devtmpfs -x devfs -x specfs'
   * || export DF_FLAGS='-T -k -l -x tmpfs -x devtmpfs -x devfs -x specfs'

# Use short hostname instead of FQDN to mark nodes online/offline.
# This prevents the
#     "Not sure how to handle node state "" on ..."
# error when machines have FQDN hostnames, but are listed with short names in the SLURM config.
   * || HOSTNAME="$HOSTNAME_S"

#######################################################################
###
### Hardware checks
###
# Set these to your correct socket, core, and thread counts.
   * || check_hw_cpuinfo {{ slurm_sockets }} {{ slurm_sockets * slurm_cores_per_socket }} {{ slurm_sockets * slurm_cores_per_socket * slurm_threads_per_core | default(1) }}

# Set these to the amount of physical RAM you have (leave the fudge factor).
   * || check_hw_physmem {{ slurm_real_memory }}MB {{ slurm_real_memory }}MB 5%

# Check specifically for free physical memory.
   * || check_hw_physmem_free 1MB

# Check for some sort of free memory of either type.
   * || check_hw_mem_free {{ [2048, (ansible_memtotal_mb | float * 0.02) | int] | min }}MB

# Checks for an active ethernet interfaces.
{% for ethernet_interface in slurm_ethernet_interfaces %}
   * || check_hw_eth {{ ethernet_interface }}
{% endfor %}

# Check the mcelog daemon for any pending errors.
   * || check_hw_mcelog

{% if gpu_count is defined and gpu_count > 0 %}
# Check the status of each nvidia GPU device
  {% for gpu_num in range(0, gpu_count|int - 1) %}
   * || check_cmd_output -t 15 -r 0 nvidia-smi -i {{ gpu_num }}
  {% endfor %}
{% endif %}

#######################################################################
###
### Filesystem checks
###
# All nodes should have their root filesystem mounted read/write.
   * || check_fs_mount_rw -f /

# Controlling TTYs are a good thing!
   * || check_fs_mount_rw -t devpts -s '/(none|devpts)/' -f /dev/pts

# Make sure the root filesystem doesn't get too full.
   * || check_fs_free / 10%

# Make sure the root filesystem has enough free inodes.
   * || check_fs_ifree / 1k

# Make sure the /var volume doesn't get too full.
   * || check_fs_free /var 5%

# Make sure the /var filesystem has enough free inodes.
   * || check_fs_ifree /var 1k

#
## Home
# 
# All UI's and compute nodes should have their home filesystem mounted read/write
# File system type is parsed from the pfs for an lfs. If 'none', then use xfs/etx4.
#
{% for lfs_item in lfs_mounts | selectattr('lfs', 'search', 'home') -%}
  {% if pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first == 'none' %}
    {% set pfs_type = '/(xfs|ext4)/' %}
  {% else %}
    {% set pfs_type = pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first %}
  {% endif %}
  {% if inventory_hostname in lfs_item.rw_machines | default([]) %}
   * || check_fs_mount_rw -f /home -t '{{ pfs_type }}'
  {% elif inventory_hostname in lfs_item.ro_machines | default([]) %}
   * || check_fs_mount_ro -f /home -t '{{ pfs_type }}'
  {% endif %}
{% endfor %}

#
## Apps
#
# All UIs and nodes should have their apps filesystem mounted read-only
# except on all-in-one Slurm machines, without a dedicated DAI, who need it mounted read/write.
# File system type is parsed from the pfs for an lfs. If 'none', then use xfs/etx4.
#
{% for lfs_item in lfs_mounts | selectattr('lfs', 'search', 'env[0-9]+$') %}
  {% if pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first == 'none' %}
    {% set pfs_type = '/(xfs|ext4)/' %}
  {% else %}
    {% set pfs_type = pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first %}
  {% endif %}
  {% if inventory_hostname in lfs_item.rw_machines | default([]) %}
   * || check_fs_mount_rw -f /apps -t '{{ pfs_type }}'
  {% elif inventory_hostname in lfs_item.ro_machines | default([]) %}
   * || check_fs_mount_ro -f /apps -t '{{ pfs_type }}'
  {% endif %}
{% endfor %}

#
## Groups
#
# All UI's and compute nodes should have their group folders mounted
# either read/write or read-only as specified for that LFS.
# File system type is parsed from the pfs for an lfs. If 'none', then use xfs/etx4.
#
{% for lfs_item in lfs_mounts | selectattr('lfs', 'search', '((tmp)|(rsc)|(prm)|(dat))[0-9]+$') %}
  {% if pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first == 'none' %}
    {% set pfs_type = '/(xfs|ext4)/' %}
  {% else %}
    {% set pfs_type = pfs_mounts | selectattr('pfs', 'equalto', lfs_item.pfs) | map(attribute='type') | first %}
  {% endif %}
  {% for group in lfs_item.groups %}
    {% if inventory_hostname in lfs_item.rw_machines | default([]) %}
   * || check_fs_mount_rw -f /groups/{{ group.name }}/{{ lfs_item.lfs }} -t '{{ pfs_type }}'
    {% elif inventory_hostname in lfs_item.ro_machines | default([]) %}
   * || check_fs_mount_ro -f /groups/{{ group.name }}/{{ lfs_item.lfs }} -t '{{ pfs_type }}'
    {% endif %}
  {% endfor %}
{% endfor %}

{% if slurm_local_disk is defined and slurm_local_disk > 0 %}
# 
# Local
#
# All compute nodes should have their {{ slurm_local_scratch_dir }} filesystem mounted read/write.
#
   * || check_fs_mount_rw -f {{ slurm_local_scratch_dir }}
{% endif %}

#######################################################################
###
### File/metadata checks
###
# These should always be directories and always be read/write/execute and sticky.
   * || check_file_test -r -w -x -d -k /tmp /var/tmp

# These should always be readable and should never be empty.
   * || check_file_test -r -s /etc/passwd /etc/group

# Assert common properties for /dev/null (which occasionally gets clobbered).
   * || check_file_test -c -r -w /dev/null /dev/zero
   * || check_file_stat -m 0666 -u 0 -g 0 -t 1 -T 3 /dev/null

# Make sure there's daily activity from the syslog.
   * || check_file_stat -n 86520 /var/log/messages

# Validate a couple important accounts in the passwd file.
   * || check_file_contents /etc/passwd "/^root:x:0:0:/" "sshd:*"


#######################################################################
###
### Process checks
###
# Everybody needs sshd running, right?  But don't use -r (restart)!
   * || check_ps_service -u root -S {% if ansible_facts['os_family'] == "RedHat" and ansible_facts['distribution_major_version'] >= "9"%}-m 'sshd:'{% endif %} sshd

# The cron daemon is another useful critter...
   * || check_ps_service -r crond

# This is only valid for RHEL6 and similar/newer systems.
   * || check_ps_service -d rsyslogd -r rsyslog

# Double your core count is a good rule of thumb for load average max.
# This should work if you place it after one of the check_hw_*() checks.
   * || check_ps_loadavg $((2*HW_CORES))
